from dotenv import load_dotenv
from document_loader import load_txt
from chunk_splitter import split_into_chunks
from vector_store import get_vector_store, save_vector_store, get_embeddings, update_path
from langchain_community.vectorstores import FAISS

from file_preprocessor import FilePreprocessor

load_dotenv()

NOTE_PATH = "test_picture.png"
preprocessor = FilePreprocessor()

def update_db():
    """Load document, split into chunks, and add to vector store"""
    print(f"ğŸ“– WczytujÄ™ dokument: {NOTE_PATH}")
    text = preprocessor.preprocess(NOTE_PATH)
    
    print("âœ‚ï¸  DzielÄ™ tekst na chunki...")
    chunks = split_into_chunks(text)
    
    print(f"ğŸ“Š Utworzono {len(chunks)} chunkÃ³w")
    
    print("ğŸ’¾ Sprawdzam bazÄ™ wektorowÄ…...")
    update_path()
    db = get_vector_store()
    
    if db is None:
        print("ğŸ†• TworzÄ™ nowÄ… bazÄ™ wektorowÄ…...")
        embeddings = get_embeddings()
        db = FAISS.from_texts(chunks, embeddings)
        print("ğŸ’¿ ZapisujÄ™ nowÄ… bazÄ™...")
    else:
        print("â• DodajÄ™ chunki do istniejÄ…cej bazy...")
        db.add_texts(chunks)
        print("ğŸ’¿ AktualizujÄ™ bazÄ™...")
    
    save_vector_store(db)
    
    print(f"âœ… PomyÅ›lnie dodano {len(chunks)} chunkÃ³w do bazy")

if __name__ == "__main__":
    update_db()